var documenterSearchIndex = {"docs":
[{"location":"#NMOpt.jl","page":"Home","title":"NMOpt.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for NMOpt.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"NMOpt.NMOpt\nNMOpt.BFGS\nNMOpt.BFGSB\nNMOpt.LBFGS\nNMOpt.LBFGSB\nNMOpt.line_search\nNMOpt.belong_to","category":"page"},{"location":"#NMOpt.NMOpt","page":"Home","title":"NMOpt.NMOpt","text":"This is the NMOpt, it contains \n\nNMOpt.BFGS\nNMOpt.BFGSB\nNMOpt.LBFGS\nNMOpt.LBFGSB\nNMOpt.belong_to\nNMOpt.line_search\n\nref:\n\nMore and Sorensen 1982 (Newton's method technical report): BFGS\nMore 1994 (Line Search Algorithms Sufficient Decrease): line search\nNocedal 1980 (Updating Quasi-Newton Matrices With Limited Storage): limited memory BFGS\nThiébaut (Optimization issues in blind deconvolution algorithms): bounded limited memory BFGS, a.k.a vmlmb\n\n\n\n\n\n","category":"module"},{"location":"#NMOpt.BFGS","page":"Home","title":"NMOpt.BFGS","text":"BFGS(X0::Array{Cdouble,1},H0::Array{Cdouble,2},Nbfgs::Int64,alpha_min::Cdouble,alpha_max::Cdouble,mu::Cdouble,F::Function,Fgrad::Function,Nsearch::Int64=50,tol::Cdouble=1.0e-8;verbose::Bool=false,path::Bool=true)\nBFGS(X0::Cdouble,H0::Cdouble,Nbfgs::Int64,alpha_min::Cdouble,alpha_max::Cdouble,mu::Cdouble,F::Function,Fgrad::Function,Nsearch::Int64=50,tol::Cdouble=1.0e-8;verbose::Bool=false,path::Bool=true)\n\ncompute ̂x̂ ∈ argmin{F(x)}  using the quasi-Newton method BFGS (see [BFGS wikipedia](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm))\n\nX0:                  starting point\nH0:                  initial inverse Hessian matrix\nNbfgs:               maximum number of iterations\nalpha_min,alpha_max: line search maximum range\nmu:                  line search parameter\nNsearch:             maximum number of iteration for the line search\nF:                   cost function to minimize (x::Array{Cdouble}->F(x))\nFgrad:               gradient of the cost function (x::Array{Cdouble}->Fgrad(x))\ntol:                 relative tolerance (stopping criteria: norm of the gradient difference (y), norm of the step (s) and cos(y,s))\n\noptional arguments:\n\n  - verbose:             verbose if set to true\n  - path:                keep track of all iterations if set to true (if false: Xpath=nothing)\n\noutput:\n\n  - X:     final state of the optimization process (in the vincinity of argmin{F(x)})\n  - H:     estimate of the inverse Hessian matrix \n  - Xpath: steps between the initial point X0 and the arrival point X, or nothing \n  - Nlast: number of iteration\n\n\n\n\n\n","category":"function"},{"location":"#NMOpt.BFGSB","page":"Home","title":"NMOpt.BFGSB","text":"BFGSB(X0::Array{Cdouble,1},H0::Array{Cdouble,2},Nbfgs::Int64,alpha_min::Cdouble,alpha_max::Cdouble,mu::Cdouble,lx::Union{Cdouble,Array{Cdouble,1}},ux::Union{Cdouble,Array{Cdouble,1}},F::Function,Fgrad::Function,Nsearch::Int64=50,tol::Cdouble=1.0e-8;verbose::Bool=false,path::Bool=true)\nBFGSB(X0::Array{Cdouble,1},H0::Array{Cdouble,2},Nbfgs::Int64,alpha_min::Cdouble,alpha_max::Cdouble,mu::Cdouble,lx::Array{Cdouble,1},ux::Array{Cdouble,1},F::Function,Fgrad::Function,Nsearch::Int64=50,tol::Cdouble=1.0e-8;verbose::Bool=false)\n\ncompute ̂x̂ ∈ argmin{F(x) | x.>= lx and x.<=ux}  using the quasi-Newton method BFGS (see [BFGS wikipedia](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm))\n\nX0:                  starting point\nH0:                  initial inverse Hessian matrix\nNbfgs:               maximum number of iterations\nalpha_min,alpha_max: line search maximum range\nmu:                  line search parameter\nNsearch:             maximum number of iteration for the line search\nF:                   cost function to minimize (x::Array{Cdouble}->F(x))\nFgrad:               gradient of the cost function (x::Array{Cdouble}->Fgrad(x))\nlx,ux:               lower and upper boundary of ̂x \ntol:                 relative tolerance (stopping criteria: norm of the gradient difference (y), norm of the step (s) and cos(y,s))\n\noptional arguments:\n\n  - verbose:             verbose if set to true\n  - path:                keep track of all iterations if set to true (if false: Xpath=nothing)\n\noutput:\n\n  - X:     final state of the optimization process (in the vincinity of argmin{F(x)})\n  - H:     estimate of the inverse Hessian matrix \n  - Xpath: steps between the initial point X0 and the arrival point X, or nothing \n  - Nlast: number of iteration\n\n\n\n\n\n","category":"function"},{"location":"#NMOpt.LBFGS","page":"Home","title":"NMOpt.LBFGS","text":"LBFGS(X0::Array{Cdouble,1},H0::Array{Cdouble,2},Nbfgs::Int64,alpha_min::Cdouble,alpha_max::Cdouble,mu::Cdouble,M::Int64,F::Function,Fgrad::Function,Nsearch::Int64=50,tol::Cdouble=1.0e-8;verbose::Bool=false,path::Bool=true)\n\ncompute ̂x̂ ∈ argmin{F(x)}  using the limitted memory quasi-Newton method L-BFGS  (see [BFGS wikipedia](https://en.wikipedia.org/wiki/Limited-memory_BFGS))\n\nX0:                  starting point\nH0:                  initial inverse Hessian matrix\nNbfgs:               maximum number of iterations\nalpha_min,alpha_max: line search maximum range\nmu:                  line search parameter\nM:                   number of stored vectors (limit memory usage compared with the computation of the inverse of the Hessian matrix)\nNsearch:             maximum number of iteration for the line search\nF:                   cost function to minimize (x::Array{Cdouble}->F(x))\nFgrad:               gradient of the cost function (x::Array{Cdouble}->Fgrad(x))\ntol:                 relative tolerance (stopping criteria: norm of the gradient difference (y), norm of the step (s) and cos(y,s))\n\noptional arguments:\n\n  - verbose: by default set to false \n  - path:    default true, keep track of all the steps between the initial point X0 and the arrival point X (if false: Xpath=nothing)\n\noutput:\n\n  - X:     final state of the optimization process (in the vincinity of argmin{F(x)})\n  - Xpath: steps between the initial point X0 and the arrival point X\n  - Nlast: number of iteration\n\n\n\n\n\n","category":"function"},{"location":"#NMOpt.LBFGSB","page":"Home","title":"NMOpt.LBFGSB","text":"LBFGSB(X0::Array{Cdouble,1},H0::Array{Cdouble,2},Nbfgs::Int64,alpha_min::Cdouble,alpha_max::Cdouble,mu::Cdouble,M::Int64,lx::Array{Cdouble,1},ux::Array{Cdouble,1},F::Function,Fgrad::Function,Nsearch::Int64=50,tol::Cdouble=1.0e-8;verbose::Bool=false,path::Bool=true)\n\ncompute ̂x̂ ∈ argmin{F(x) | x.>= lx and x.<=ux}  using the limitted memory quasi-Newton method L-BFGS  (see [BFGS wikipedia](https://en.wikipedia.org/wiki/Limited-memory_BFGS))\n\nX0:                  starting point\nH0:                  initial inverse Hessian matrix\nNbfgs:               maximum number of iterations\nalpha_min,alpha_max: line search maximum range\nmu:                  line search parameter\nM:                   number of stored vectors (limit memory usage compared with the computation of the inverse of the Hessian matrix)\nNsearch:             maximum number of iteration for the line search\nF:                   cost function to minimize (x::Array{Cdouble}->F(x))\nFgrad:               gradient of the cost function (x::Array{Cdouble}->Fgrad(x))\nlx,ux:               lower and upper boundary of ̂x \ntol:                 relative tolerance (stopping criteria: norm of the gradient difference (y), norm of the step (s) and cos(y,s))\n\noptional arguments:\n\n  - verbose: by default set to false \n  - path:    default true, keep track of all the steps between the initial point X0 and the arrival point X (if false: Xpath=nothing)\n\noutput:\n\n  - X:     final state of the optimization process (in the vincinity of argmin{F(x)})\n  - Xpath: steps between the initial point X0 and the arrival point X\n  - Nlast: number of iteration\n\n\n\n\n\n","category":"function"},{"location":"#NMOpt.line_search","page":"Home","title":"NMOpt.line_search","text":"line_search(x_::Array{Cdouble,1},p_::Array{Cdouble,1},alpha_min::Cdouble,alpha_max::Cdouble,mu::Cdouble,F::Function,Fgrad::Function,Nsearch::Int64=50;verbose::Bool=false)\n\ncompute ̂̂α ∈ argmin{F(x+α*p)}\n\ninputs:\n\n  - x: point x\n  - p: search direction \n  - α_min, and α_max:  minimum and maximum values for α\n  - μ: weight for the criterion F(x+α*p)<F(x)+μ*α ⟨p,∂F/∂x(x)⟩, and  |⟨p,∂F/∂x(x+α*p)⟩|⩽μ|⟨p,∂F/∂x(x)⟩|\n  - F and Fgrad: cost function and its gradient \n  - Nsearch: maximum number of iteration for the line search \n\noptional argument:\n\n  - verbose: verbose if true \n\noutput: \n\n  - α within the vincinity of the ̂α\n\n\n\n\n\n","category":"function"},{"location":"#NMOpt.belong_to","page":"Home","title":"NMOpt.belong_to","text":"belong_to(x_::Array{Cdouble,1},p_::Array{Cdouble,1},alpha_::Cdouble,mu_::Cdouble,F::Function,Fgrad::Function)\nbelong_to(x_::Cdouble,p_::Cdouble,alpha_::Cdouble,mu_::Cdouble,F::Function,Fgrad::Function)\n\ncheck the criterion \n\nF(x+α*p)<F(x)+μ*α ⟨p,∂F/∂x(x)⟩\n\n|⟨p,∂F/∂x(x+α*p)⟩|⩽μ|⟨p,∂F/∂x(x)⟩|\n\nreturn true if both criterion are met, false otherwise\n\n\n\n\n\n","category":"function"}]
}
